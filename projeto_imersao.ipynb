{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSzza+NurHJl5Y024JVsfP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabino975/projeto_imersao_alura_05/blob/main/projeto_imersao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instala a biblioteca Generative Ai\n",
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "WbUPH_EgXVIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação das bibliotecas utilizadas no projeto\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "b6aL8UOlH-Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuração da chave utilizada pela biblioteca Generative Ai\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "7aJCNYE5IJfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuração dos parâmetros utilizados pela biblioteca Generative Ai\n",
        "#Parâmetros de geração\n",
        "generation_config = {\n",
        "  \"candidate_count\": 1,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 0,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"temperature\": 0.5,\n",
        "  \"response_mime_type\": \"application/json\"\n",
        "}\n",
        "#Parâmetros de segurança\n",
        "safety_settings = [\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "]\n",
        "# Instruções iniciais para o modelo montar o resultado da pesquisa em um formato definido\n",
        "system_instruction = \"Você é um assistente de pesquisa.  \\nDeve pesquisar por temas relacionados as palavras chave informadas.\\nO resultado da pesquisa deve ser exibido no seguinte formato:\\n\\nTítulo: Inteligência Artificial (IA) e Machine Learning \\nDescrição: Como a IA está impulsionando a tomada de decisão em carros autônomos, incluindo navegação, detecção de objetos e aprendizado com experiências passadas.\\n\\nTítulo: Sensores e Sistemas de Percepção\\nDescrição: Radar, LiDAR, câmeras e sensores ultrassônicos que permitem aos carros autônomos \\\"ver\\\" e interpretar o ambiente.\\n\\nTítulo: \\nDescrição:\\n\"\n",
        "\n"
      ],
      "metadata": {
        "id": "l62vC7HmIZII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lista os modelos relacionados a geração de conteúdo\n",
        "print(\"-------------------------------------------------------------------------\")\n",
        "print(\"---------------------- generate content models ---------------------------\")\n",
        "print(\"-------------------------------------------------------------------------\")\n",
        "for m in genai.list_models():\n",
        "  if('generateContent') in m.supported_generation_methods:\n",
        "    print(m.name)\n",
        "print(\"-------------------------------------------------------------------------\")\n",
        "print(\"---------------------- embed content models -----------------------------\")\n",
        "print(\"-------------------------------------------------------------------------\")\n",
        "#lista os modelos para pesquisa de conteúdo\n",
        "for m in genai.list_models():\n",
        "  if('embedContent') in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "id": "hyrelsQaYEjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialização da biblioteca generative ia\n",
        "generative_model = \"models/gemini-1.5-pro-latest\"\n",
        "model = genai.GenerativeModel(model_name=generative_model,\n",
        "                              generation_config=generation_config,\n",
        "                              system_instruction=system_instruction,\n",
        "                              safety_settings=safety_settings)"
      ],
      "metadata": {
        "id": "UW0InHlcI4UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialização do chat\n",
        "chat = model.start_chat(history=[])"
      ],
      "metadata": {
        "id": "bu2qncFDKWTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pesquisa conforme as palavras chave informadas\n",
        "prompt = input(\"Informe as palavras chave, separadas por vírgula: \")\n",
        "response = chat.send_message(prompt)"
      ],
      "metadata": {
        "id": "K69Xgo6VKgBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega o resultado da pesquisa\n",
        "data = json.loads(response.text)\n",
        "\n",
        "# Cria o dataframe\n",
        "df = pd.DataFrame(data)\n",
        "df.columns = [\"title\", \"content\"]\n",
        "\n",
        "# Exibe o dataframe\n",
        "df"
      ],
      "metadata": {
        "id": "7NCSKjXsKzBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modelo de pesquisa de conteúdo\n",
        "embedding_model = \"models/embedding-001\""
      ],
      "metadata": {
        "id": "ggcdoyoaZpTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#função que monta a coluna para pesquisa de conteúdo\n",
        "def embedding_column_value(title, text):\n",
        "  return genai.embed_content(model=embedding_model,\n",
        "                             content=text,\n",
        "                             title=title,\n",
        "                             task_type=\"RETRIEVAL_DOCUMENT\")[\"embedding\"]"
      ],
      "metadata": {
        "id": "IphqueM7LjEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preenche o campo search no dataframe com os dados de pesquisa\n",
        "df[\"search\"] = df.apply(lambda row: embedding_column_value(row[\"title\"], row[\"content\"]), axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "jyIftekdahY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# função utilizada para consulta de dados, de acordo com a pergunta feita\n",
        "def search_data(search, base, model):\n",
        "  result = genai.embed_content(model=model,\n",
        "                               content=search,\n",
        "                               task_type=\"RETRIEVAL_QUERY\")[\"embedding\"]\n",
        "\n",
        "  scalar_result = np.dot(np.stack(df[\"search\"]), result)\n",
        "\n",
        "  index = np.argmax(scalar_result)\n",
        "  if(index > -1):\n",
        "    return df.iloc[index][\"content\"]\n",
        "  else:\n",
        "    return f\"Conteúdo não localizado contendo a pesquisa {search}\""
      ],
      "metadata": {
        "id": "nQh_IbGIdWuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = input(\"Informe os dados da pesquisa: \")\n",
        "\n",
        "result = search_data(search, df, embedding_model)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "T3x9NiVHd9lZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}